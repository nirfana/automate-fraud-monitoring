[2024-06-30T09:33:37.880+0000] {taskinstance.py:1171} INFO - Dependencies all met for <TaskInstance: Automate_cleaning.load_raw_data scheduled__2024-06-21T06:30:00+00:00 [queued]>
[2024-06-30T09:33:37.937+0000] {taskinstance.py:1171} INFO - Dependencies all met for <TaskInstance: Automate_cleaning.load_raw_data scheduled__2024-06-21T06:30:00+00:00 [queued]>
[2024-06-30T09:33:37.938+0000] {taskinstance.py:1368} INFO - 
--------------------------------------------------------------------------------
[2024-06-30T09:33:37.939+0000] {taskinstance.py:1369} INFO - Starting attempt 1 of 1
[2024-06-30T09:33:37.942+0000] {taskinstance.py:1370} INFO - 
--------------------------------------------------------------------------------
[2024-06-30T09:33:38.034+0000] {taskinstance.py:1389} INFO - Executing <Task(PythonOperator): load_raw_data> on 2024-06-21 06:30:00+00:00
[2024-06-30T09:33:38.054+0000] {standard_task_runner.py:52} INFO - Started process 218 to run task
[2024-06-30T09:33:38.058+0000] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'Automate_cleaning', 'load_raw_data', 'scheduled__2024-06-21T06:30:00+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/automate_clean_DAG.py', '--cfg-path', '/tmp/tmp83rcvx5x', '--error-file', '/tmp/tmpcck7_whe']
[2024-06-30T09:33:38.069+0000] {standard_task_runner.py:80} INFO - Job 5: Subtask load_raw_data
[2024-06-30T09:33:38.279+0000] {task_command.py:371} INFO - Running <TaskInstance: Automate_cleaning.load_raw_data scheduled__2024-06-21T06:30:00+00:00 [running]> on host 7ad299a7fc17
[2024-06-30T09:33:38.607+0000] {taskinstance.py:1583} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=devi_nirfana
AIRFLOW_CTX_DAG_ID=Automate_cleaning
AIRFLOW_CTX_TASK_ID=load_raw_data
AIRFLOW_CTX_EXECUTION_DATE=2024-06-21T06:30:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-06-21T06:30:00+00:00
[2024-06-30T09:34:15.289+0000] {python.py:173} INFO - Done. Returned value was: None
[2024-06-30T09:34:15.422+0000] {taskinstance.py:1412} INFO - Marking task as SUCCESS. dag_id=Automate_cleaning, task_id=load_raw_data, execution_date=20240621T063000, start_date=20240630T093337, end_date=20240630T093415
[2024-06-30T09:34:15.540+0000] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-06-30T09:34:15.828+0000] {local_task_job.py:279} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-30T09:38:21.869+0000] {taskinstance.py:1171} INFO - Dependencies all met for <TaskInstance: Automate_cleaning.load_raw_data scheduled__2024-06-21T06:30:00+00:00 [queued]>
[2024-06-30T09:38:21.904+0000] {taskinstance.py:1171} INFO - Dependencies all met for <TaskInstance: Automate_cleaning.load_raw_data scheduled__2024-06-21T06:30:00+00:00 [queued]>
[2024-06-30T09:38:21.907+0000] {taskinstance.py:1368} INFO - 
--------------------------------------------------------------------------------
[2024-06-30T09:38:21.908+0000] {taskinstance.py:1369} INFO - Starting attempt 1 of 1
[2024-06-30T09:38:21.908+0000] {taskinstance.py:1370} INFO - 
--------------------------------------------------------------------------------
[2024-06-30T09:38:21.958+0000] {taskinstance.py:1389} INFO - Executing <Task(PythonOperator): load_raw_data> on 2024-06-21 06:30:00+00:00
[2024-06-30T09:38:21.971+0000] {standard_task_runner.py:52} INFO - Started process 212 to run task
[2024-06-30T09:38:21.986+0000] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'Automate_cleaning', 'load_raw_data', 'scheduled__2024-06-21T06:30:00+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/automate_clean_DAG.py', '--cfg-path', '/tmp/tmpxq47bxvt', '--error-file', '/tmp/tmpaesj02vh']
[2024-06-30T09:38:21.991+0000] {standard_task_runner.py:80} INFO - Job 5: Subtask load_raw_data
[2024-06-30T09:38:22.209+0000] {task_command.py:371} INFO - Running <TaskInstance: Automate_cleaning.load_raw_data scheduled__2024-06-21T06:30:00+00:00 [running]> on host 3e5e5d011624
[2024-06-30T09:38:22.450+0000] {taskinstance.py:1583} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=devi_nirfana
AIRFLOW_CTX_DAG_ID=Automate_cleaning
AIRFLOW_CTX_TASK_ID=load_raw_data
AIRFLOW_CTX_EXECUTION_DATE=2024-06-21T06:30:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-06-21T06:30:00+00:00
[2024-06-30T09:38:51.708+0000] {python.py:173} INFO - Done. Returned value was: None
[2024-06-30T09:38:51.952+0000] {taskinstance.py:1412} INFO - Marking task as SUCCESS. dag_id=Automate_cleaning, task_id=load_raw_data, execution_date=20240621T063000, start_date=20240630T093821, end_date=20240630T093851
[2024-06-30T09:38:52.183+0000] {local_task_job.py:156} INFO - Task exited with return code 0
[2024-06-30T09:38:52.318+0000] {local_task_job.py:279} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-30T09:55:59.434+0000] {taskinstance.py:1171} INFO - Dependencies all met for <TaskInstance: Automate_cleaning.load_raw_data scheduled__2024-06-21T06:30:00+00:00 [queued]>
[2024-06-30T09:55:59.583+0000] {taskinstance.py:1171} INFO - Dependencies all met for <TaskInstance: Automate_cleaning.load_raw_data scheduled__2024-06-21T06:30:00+00:00 [queued]>
[2024-06-30T09:55:59.584+0000] {taskinstance.py:1368} INFO - 
--------------------------------------------------------------------------------
[2024-06-30T09:55:59.595+0000] {taskinstance.py:1369} INFO - Starting attempt 1 of 1
[2024-06-30T09:55:59.599+0000] {taskinstance.py:1370} INFO - 
--------------------------------------------------------------------------------
[2024-06-30T09:55:59.746+0000] {taskinstance.py:1389} INFO - Executing <Task(PythonOperator): load_raw_data> on 2024-06-21 06:30:00+00:00
[2024-06-30T09:55:59.791+0000] {standard_task_runner.py:52} INFO - Started process 346 to run task
[2024-06-30T09:55:59.789+0000] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'Automate_cleaning', 'load_raw_data', 'scheduled__2024-06-21T06:30:00+00:00', '--job-id', '6', '--raw', '--subdir', 'DAGS_FOLDER/automate_clean_DAG.py', '--cfg-path', '/tmp/tmp93peud1s', '--error-file', '/tmp/tmp7a3oypds']
[2024-06-30T09:55:59.803+0000] {standard_task_runner.py:80} INFO - Job 6: Subtask load_raw_data
[2024-06-30T09:56:00.027+0000] {task_command.py:371} INFO - Running <TaskInstance: Automate_cleaning.load_raw_data scheduled__2024-06-21T06:30:00+00:00 [running]> on host 80cef885917b
[2024-06-30T09:56:00.475+0000] {taskinstance.py:1583} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=devi_nirfana
AIRFLOW_CTX_DAG_ID=Automate_cleaning
AIRFLOW_CTX_TASK_ID=load_raw_data
AIRFLOW_CTX_EXECUTION_DATE=2024-06-21T06:30:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-06-21T06:30:00+00:00
[2024-06-30T09:56:02.134+0000] {taskinstance.py:1902} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1803, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 719, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.DuplicateTable: relation "fraud-data-table" already exists


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 171, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 189, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/automate_clean_DAG.py", line 44, in load_to_postgres
    df.to_sql('fraud-data-table', conn, index=False, if_exists='replace')
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/core/generic.py", line 2882, in to_sql
    method=method,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/sql.py", line 728, in to_sql
    **engine_kwargs,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/sql.py", line 1758, in to_sql
    dtype=dtype,
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/sql.py", line 1650, in prep_table
    table.create()
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/sql.py", line 861, in create
    self._execute_create()
  File "/home/airflow/.local/lib/python3.7/site-packages/pandas/io/sql.py", line 853, in _execute_create
    self.table.create()
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/schema.py", line 950, in create
    bind._run_ddl_visitor(ddl.SchemaGenerator, self, checkfirst=checkfirst)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 2113, in _run_ddl_visitor
    visitorcallable(self.dialect, self, **kwargs).traverse_single(element)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/visitors.py", line 524, in traverse_single
    return meth(obj, **kw)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/ddl.py", line 895, in visit_table
    include_foreign_key_constraints,  # noqa
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1289, in execute
    return meth(self, multiparams, params, _EMPTY_EXECUTION_OPTS)
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/ddl.py", line 78, in _execute_on_connection
    self, multiparams, params, execution_options
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1387, in _execute_ddl
    compiled,
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    e, statement, parameters, cursor, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 2027, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/base.py", line 1803, in _execute_context
    cursor, statement, parameters, context
  File "/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/default.py", line 719, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.DuplicateTable) relation "fraud-data-table" already exists

[SQL: 
CREATE TABLE "fraud-data-table" (
	"Transaction ID" TEXT, 
	"Date" TEXT, 
	"Day of Week" TEXT, 
	"Time" BIGINT, 
	"Type of Card" TEXT, 
	"Entry Mode" TEXT, 
	"Amount" TEXT, 
	"Type of Transaction" TEXT, 
	"Merchant Group" TEXT, 
	"Country of Transaction" TEXT, 
	"Shipping Address" TEXT, 
	"Country of Residence" TEXT, 
	"Gender" TEXT, 
	"Age" FLOAT(53), 
	"Bank" TEXT, 
	"Fraud" BIGINT
)

]
(Background on this error at: https://sqlalche.me/e/14/f405)
[2024-06-30T09:56:02.209+0000] {taskinstance.py:1412} INFO - Marking task as FAILED. dag_id=Automate_cleaning, task_id=load_raw_data, execution_date=20240621T063000, start_date=20240630T095559, end_date=20240630T095602
[2024-06-30T09:56:02.279+0000] {standard_task_runner.py:97} ERROR - Failed to execute job 6 for task load_raw_data ((psycopg2.errors.DuplicateTable) relation "fraud-data-table" already exists

[SQL: 
CREATE TABLE "fraud-data-table" (
	"Transaction ID" TEXT, 
	"Date" TEXT, 
	"Day of Week" TEXT, 
	"Time" BIGINT, 
	"Type of Card" TEXT, 
	"Entry Mode" TEXT, 
	"Amount" TEXT, 
	"Type of Transaction" TEXT, 
	"Merchant Group" TEXT, 
	"Country of Transaction" TEXT, 
	"Shipping Address" TEXT, 
	"Country of Residence" TEXT, 
	"Gender" TEXT, 
	"Age" FLOAT(53), 
	"Bank" TEXT, 
	"Fraud" BIGINT
)

]
(Background on this error at: https://sqlalche.me/e/14/f405); 346)
[2024-06-30T09:56:02.338+0000] {local_task_job.py:156} INFO - Task exited with return code 1
[2024-06-30T09:56:02.606+0000] {local_task_job.py:279} INFO - 0 downstream tasks scheduled from follow-on schedule check
